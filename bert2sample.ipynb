{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Id Relationship Role  \\\n",
      "0    1047538821            mentee   \n",
      "1    1047497874            mentee   \n",
      "2         10698            mentee   \n",
      "3    1047549000            mentee   \n",
      "4    1047592264            mentee   \n",
      "..          ...               ...   \n",
      "188  1047585221            mentee   \n",
      "189  1047583535            mentor   \n",
      "190  1047627569            mentee   \n",
      "191  1047501471            mentee   \n",
      "192       10988            mentor   \n",
      "\n",
      "                                     Nearest Neighbors  \n",
      "0    [(1047516619, 1.0, mentee), (1047503059, 1.0, ...  \n",
      "1    [(1047498727, 1.0, mentee), (1047501363, 1.0, ...  \n",
      "2    [(10988, 1.0, mentor), (10716, 1.0, mentee), (...  \n",
      "3    [(1047554104, 1.0, mentee), (1047588327, 1.0, ...  \n",
      "4    [(1047593954, 1.0, mentee), (1047541619, 1.0, ...  \n",
      "..                                                 ...  \n",
      "188  [(1047593954, 1.0, mentee), (1047541038, 1.0, ...  \n",
      "189  [(1047549915, 0.98, mentor), (1047567699, 0.98...  \n",
      "190  [(1047541032, 1.0, mentee), (1047554195, 1.0, ...  \n",
      "191  [(1047499341, 1.0, mentee), (10330, 1.0, mento...  \n",
      "192  [(10698, 1.0, mentee), (11588, 1.0, mentor), (...  \n",
      "\n",
      "[193 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#TODO add weights to the profile columns, compare the results of the embeddings (eg. confirm similarity score between ID is legit)\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "df = pd.read_csv(\"../../../clean_data/profiles.csv\", encoding='utf-8')\n",
    "df = df.astype(str)\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "df['Profile'] = df.drop(columns=['Id', 'Created at', 'Relationship Role', 'Total Mentees', 'Number of Messages Sent', 'Resource Clicks', 'Courses Clicks']).agg(' '.join, axis=1)\n",
    "\n",
    "inputs = df['Profile'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
    "\n",
    "inputs = inputs.tolist()\n",
    "max_len = max(len(seq) for seq in inputs)\n",
    "padded_inputs = [seq + [0] * (max_len - len(seq)) for seq in inputs]\n",
    "\n",
    "input_ids = torch.tensor(padded_inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "embeddings = embeddings.mean(dim=1).numpy()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "X_reduced = svd.fit_transform(embeddings)\n",
    "\n",
    "cos_sim_matrix = cosine_similarity(X_reduced)\n",
    "\n",
    "k = 5\n",
    "knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "knn.fit(X_reduced)\n",
    "\n",
    "distances, indices = knn.kneighbors(X_reduced)\n",
    "\n",
    "results = []\n",
    "for i, profile in enumerate(df['Profile']):\n",
    "    nearest_neighbors = [(df.iloc[indices[i][j]]['Id'], round(1 - distances[i][j], 2), df.iloc[indices[i][j]]['Relationship Role']) for j in range(1, k)]\n",
    "    result = {\n",
    "        'Id': df.iloc[i]['Id'],\n",
    "        #'Profile': profile,\n",
    "        'Relationship Role': df.iloc[i]['Relationship Role'],\n",
    "        'Nearest Neighbors': nearest_neighbors\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "#results_df.to_csv(\"results_rec.csv\", index=False)\n",
    "\n",
    "#print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2471,  0.5581,  0.4856,  ..., -0.3383,  0.4712, -0.5496],\n",
      "         [-0.9043,  0.1023,  0.3905,  ..., -0.2531,  0.3107,  0.1530],\n",
      "         [-0.9974,  0.0594,  0.3350,  ..., -0.2259, -0.1157,  0.2946],\n",
      "         ...,\n",
      "         [ 0.4776,  0.3691,  0.5112,  ..., -0.4778,  0.2328, -1.2041],\n",
      "         [ 0.3487,  0.4768,  0.5018,  ..., -0.5214,  0.1758, -1.1651],\n",
      "         [ 0.1418,  0.6547,  0.6327,  ..., -0.4767,  0.1373, -1.2286]],\n",
      "\n",
      "        [[-0.2218,  0.5102,  0.0278,  ..., -0.5992,  0.4269, -0.7260],\n",
      "         [-0.2835,  1.0011,  0.5905,  ..., -0.7353,  0.4000,  0.2702],\n",
      "         [-0.7124,  0.0316,  0.9288,  ..., -0.0165,  0.2927,  0.9949],\n",
      "         ...,\n",
      "         [ 0.2611,  0.2666,  0.3623,  ..., -0.6006, -0.2367, -1.1620],\n",
      "         [ 0.4256,  0.1841,  0.4608,  ..., -0.6418, -0.1705, -1.2568],\n",
      "         [ 0.1303,  0.5525,  0.5782,  ..., -0.9839, -0.1581, -1.4536]],\n",
      "\n",
      "        [[-0.1543,  0.5035,  0.0646,  ..., -0.4356,  0.2956, -0.9883],\n",
      "         [-0.5467,  0.1428,  0.2016,  ..., -0.0600,  1.0346,  0.4549],\n",
      "         [-1.3164,  0.5711,  1.0443,  ..., -0.1843,  0.7601,  1.1221],\n",
      "         ...,\n",
      "         [ 0.5028,  0.1334,  0.3715,  ..., -0.5060, -0.2032, -1.2775],\n",
      "         [ 0.5692,  0.0806,  0.4083,  ..., -0.5830, -0.1453, -1.3386],\n",
      "         [ 0.2151,  0.4279,  0.4285,  ..., -0.8497, -0.0827, -1.5542]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4407,  0.5513, -0.3969,  ..., -0.7154,  0.6367, -0.6288],\n",
      "         [-0.1604,  0.6590,  0.3340,  ..., -0.8902,  0.4147, -0.2009],\n",
      "         [-0.1629,  1.0851,  0.9657,  ..., -0.6788, -0.0982, -0.0267],\n",
      "         ...,\n",
      "         [ 0.2051,  0.1898,  0.3165,  ..., -0.5464, -0.2829, -1.1138],\n",
      "         [ 0.2514,  0.0880,  0.3721,  ..., -0.5623, -0.2509, -1.1639],\n",
      "         [-0.0297,  0.4124,  0.4337,  ..., -0.9274, -0.1874, -1.4264]],\n",
      "\n",
      "        [[-0.1947,  0.5231,  0.0745,  ..., -0.4753,  0.3445, -0.9274],\n",
      "         [-0.4150,  0.8967,  0.2461,  ..., -0.1638, -0.0776,  0.2903],\n",
      "         [-0.3325,  0.7191,  0.4573,  ..., -0.1408, -0.4836,  0.3238],\n",
      "         ...,\n",
      "         [ 0.4919,  0.1422,  0.3705,  ..., -0.5114, -0.1870, -1.2575],\n",
      "         [ 0.5693,  0.0773,  0.4096,  ..., -0.5785, -0.1316, -1.3168],\n",
      "         [ 0.2072,  0.4209,  0.4441,  ..., -0.8412, -0.0726, -1.5265]],\n",
      "\n",
      "        [[-0.1490,  0.5094,  0.0792,  ..., -0.4451,  0.2927, -0.9786],\n",
      "         [-0.6484,  0.4502,  0.3021,  ..., -0.2382,  1.4516,  0.2943],\n",
      "         [ 0.3603,  0.4976,  0.6042,  ..., -0.4786,  0.2082, -0.1503],\n",
      "         ...,\n",
      "         [ 0.4965,  0.1172,  0.3786,  ..., -0.5110, -0.1985, -1.2675],\n",
      "         [ 0.5592,  0.0584,  0.4151,  ..., -0.5874, -0.1363, -1.3255],\n",
      "         [ 0.2045,  0.4076,  0.4293,  ..., -0.8479, -0.0760, -1.5392]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#embeddings \n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "df = pd.read_csv(\"../../../clean_data/profiles.csv\", encoding='utf-8')\n",
    "df = df.astype(str)\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "df['Profile'] = df.drop(columns=['Id', 'Created at', 'Relationship Role', 'Total Mentees', 'Number of Messages Sent', 'Resource Clicks', 'Courses Clicks']).agg(' '.join, axis=1)\n",
    "\n",
    "inputs = df['Profile'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
    "\n",
    "inputs = inputs.tolist()\n",
    "max_len = max(len(seq) for seq in inputs)\n",
    "padded_inputs = [seq + [0] * (max_len - len(seq)) for seq in inputs]\n",
    "\n",
    "input_ids = torch.tensor(padded_inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m [seq \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq)) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m     23\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(padded_inputs)\n\u001b[0;32m---> 24\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#embeddings + svd + cosine matching \n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "df = pd.read_csv(\"../../../clean_data/profiles.csv\", encoding='utf-8')\n",
    "df = df.astype(str)\n",
    "df['Profile'] = df.drop(columns=['Id', 'Created at', 'Relationship Role', 'Total Mentees', 'Number of Messages Sent', 'Resource Clicks', 'Courses Clicks']).agg(' '.join, axis=1)\n",
    "\n",
    "inputs = df['Profile'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
    "\n",
    "inputs = inputs.tolist()\n",
    "max_len = max(len(seq) for seq in inputs)\n",
    "padded_inputs = [seq + [0] * (max_len - len(seq)) for seq in inputs]\n",
    "\n",
    "input_ids = torch.tensor(padded_inputs)\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "\n",
    "embeddings = embeddings.mean(dim=1).numpy()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "X_reduced = svd.fit_transform(embeddings)\n",
    "\n",
    "cos_sim_matrix = cosine_similarity(X_reduced)\n",
    "\n",
    "k = 5\n",
    "knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "knn.fit(X_reduced)\n",
    "\n",
    "distances, indices = knn.kneighbors(X_reduced)\n",
    "\n",
    "results = []\n",
    "for i, profile in enumerate(df['Profile']):\n",
    "    nearest_neighbors = [(df['Id'][indices[i][j]], round(1 - distances[i][j], 2), df['Relationship Role'][indices[i][j]]) for j in range(1, k)]\n",
    "    result = {\n",
    "        'Id': df['Id'][i],\n",
    "        'Profile': profile,\n",
    "        'Relationship Role': df['Relationship Role'][i],\n",
    "        'Nearest Neighbors': nearest_neighbors\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "#results_df.to_csv(\"results_rec.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ementoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
